{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "102b7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from config import DOWNLOAD_DIR, INPUT_DIR\n",
    "import luts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7decdea",
   "metadata": {},
   "source": [
    "### Merge datasets, create daily sum of total precipitation\n",
    "\n",
    "We need to merge the three (sometimes two?) datasets into a single dataframe.  Then, we need to produce total sums for each day.  Source data will contain a [mix of ERA5 and ERA5T data](https://confluence.ecmwf.int/display/CUSF/ERA5+CDS+requests+which+return+a+mixture+of+ERA5+and+ERA5T+data), so we need to do this to get a consistent set of data:\n",
    " \n",
    "### Our algorithm for merging/fusing data needs to go like this,\n",
    "\n",
    "\n",
    " 1. Some data files may not have `expver` present at all, such as older data which are outside the window of modification for the rolling ERA5 updates.  Use those data as-is.\n",
    " 1. If `expver` has both values 1 and 5 (a mix of ERA5 and ERA5T data), choose `expver == 1` where possible, and `expver == 5` for timesteps when `expver == 1` is missing.\n",
    " 1. For recent data, only `expver` value 5 is present.\n",
    "\n",
    "#### Notes/Questions?\n",
    "\n",
    " * Current month may be only ERA5T / `expver = 5`\n",
    " * Prior months in the same year may have mix of `expver` 5 and 1.\n",
    " * Prior year doesn't have `expver`.\n",
    "\n",
    "\n",
    "Needs to be robust to the presence/absence of expver.\n",
    "\n",
    "Goal is that a dataframe taken for a single point looks correct with data for every timestep (no dupes, no missing).\n",
    "\n",
    "## TODO: make this work properly for January when we may only have 2 data files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f7a67",
   "metadata": {},
   "source": [
    "Define a function to look for the three files for each variable in the input directory and make a complete file of the previous year's worth of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff6c7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_dataset(input_dir, varname):\n",
    "    varname_prefix = luts.varname_prefix_lu[varname]\n",
    "    prior_year = xr.open_dataset(input_dir.joinpath(f\"{varname_prefix}_previous_year.nc\"))\n",
    "    current_year = xr.open_dataset(input_dir.joinpath(f\"{varname_prefix}_current_year.nc\"))\n",
    "    current_month = xr.open_dataset(input_dir.joinpath(f\"{varname_prefix}_current_month.nc\"))\n",
    "    \n",
    "    # Just want to make sure our assumption that data with expver values of 1 and 5 are\n",
    "    #  mutually exclusive and exhaustive\n",
    "    assert np.all(\n",
    "        np.isnan(current_year[varname].sel(expver=5)) == ~np.isnan(current_year[varname].sel(expver=1))\n",
    "    )\n",
    "    assert np.all(\n",
    "        ~np.isnan(current_year[varname].sel(expver=5)) == np.isnan(current_year[varname].sel(expver=1))\n",
    "    )\n",
    "    \n",
    "    # Select the data for each expver value and combine to get a complete continuous set of data:\n",
    "    current_year_fix = xr.merge([\n",
    "        current_year[varname].sel(expver=1).drop(\"expver\"),\n",
    "        current_year[varname].sel(expver=5).drop(\"expver\")\n",
    "    ])\n",
    "    assert ~np.any(np.isnan(current_year_fix[varname]).values)\n",
    "    \n",
    "    # merge with other dataset since they all share the same coordinate variables now\n",
    "    hourly_ds = xr.merge([\n",
    "        prior_year,\n",
    "        current_year_fix,\n",
    "        current_month,\n",
    "    ])\n",
    "    # resample ohurly to daily\n",
    "    if varname in [\"tp\"]:\n",
    "        # total precip should be summed\n",
    "        daily_ds = hourly_ds.resample(time=\"1D\").sum()\n",
    "    elif varname in [\"sd\", \"swvl1\"]:\n",
    "        # TO DO : check how the daily values of these variables should be calculated from hourly\n",
    "        daily_ds = hourly_ds.resample(time=\"1D\").mean()\n",
    "        \n",
    "    return daily_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e4d27",
   "metadata": {},
   "source": [
    "Derive the yearly datasets from the downloads and combine into a single xarray dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9841a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = DOWNLOAD_DIR.joinpath(\"inputs\")\n",
    "\n",
    "ds = xr.combine_by_coords(\n",
    "    [assemble_dataset(input_dir, varname) for varname in [\"tp\", \"sd\"]], \n",
    "    combine_attrs=\"drop_conflicts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522be41e",
   "metadata": {},
   "source": [
    "So we really will only want data as far back as 1 year from most recent data we have (right?), so we can filter those days out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d841cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = ds.time[-1]\n",
    "start_time = ds.time[-365]\n",
    "# ensure that this is indeed 365 days (time diff is nanoseconds)\n",
    "assert (end_time - start_time) / 86400E9\n",
    "ds = ds.sel(time=slice(start_time, end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d07a76",
   "metadata": {},
   "source": [
    "Now we have a single dataset consisting of all* downloaded variables for deriving all* of the indices.\n",
    "\n",
    "*(not really all of them yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519d6a0",
   "metadata": {},
   "source": [
    "## Indices\n",
    "\n",
    "I know we have another notebook for processing these but we might consider just doing the preprocessing and processing in one go because the preprocessing is rather quick.\n",
    "\n",
    "We will compute the following indices:\n",
    "\n",
    "* SPI - standardized precipitation index\n",
    "* SPEI (can ERA 5 do this?) â€“ standardized potential evaporation index\n",
    "* Total Precip\n",
    "* % of Normal Precipitation\n",
    "* SWE - snow water equivalent\n",
    "* SWE % of Normal \n",
    "* Soil Moisture Deficit\n",
    "\n",
    "These indices will be computed for the following time periods:\n",
    "\n",
    "* 30 day\n",
    "* 60 day\n",
    "* 90 day\n",
    "* 180 day\n",
    "* 365 day\n",
    "\n",
    "We may want to have the 5 time intervals for repeated use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e9714360",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [30, 60 , 90, 180, 365]\n",
    "# also will be referencing the timestamps fairly often\n",
    "times = ds.time.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b161419",
   "metadata": {},
   "source": [
    "Also create a dict for writing results maybe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a8829050",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eae9c0",
   "metadata": {},
   "source": [
    "### Total precip\n",
    "\n",
    "This one should be easy? Just sum over each time interval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c64ef4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"tp\"\n",
    "indices[index] = {}\n",
    "for i in intervals:\n",
    "    indices[index][str(i)] = ds[index].sel(time=slice(times[-(i)], times[-1])).sum(dim=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2bb95",
   "metadata": {},
   "source": [
    "### % of Normal\n",
    "\n",
    "So now this one should calculate the percent of the climatologies that the above total precip values represent over the same time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e9d76b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"pntp\"\n",
    "indices[index] = {}\n",
    "with xr.open_dataset(INPUT_DIR.joinpath(\"era5_daily_tp_climatology_1981_2020_leap.nc\")) as tp_clim_ds:\n",
    "    # need to remap longitude coordinates from [180, 360] to [-180, 0]\n",
    "    tp_clim_ds = tp_clim_ds.assign_coords(longitude=(tp_clim_ds.longitude.values) - 360)\n",
    "    for i in intervals:\n",
    "        # corresponding time slice for climotology dataset\n",
    "        clim_time_sl = slice(\n",
    "            pd.Timestamp(times[-i]).dayofyear, pd.Timestamp(times[-1]).dayofyear\n",
    "        )\n",
    "        clim_tp = tp_clim_ds.sel(\n",
    "            time=clim_time_sl,\n",
    "            latitude=tp_clim_ds.latitude.values,\n",
    "        ).sum(dim=\"time\")\n",
    "        indices[index][str(i)] = (clim_tp[\"tp\"] / indices[\"tp\"][str(i)]) * 100\n",
    "        indices[index][str(i)].name = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bec3752",
   "metadata": {},
   "source": [
    "### Snow water equivalent (SWE)\n",
    "\n",
    "#### TO:DO confirm aggregation method over the intervals (currently mean but this is just a guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1f419e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"swe\"\n",
    "\n",
    "indices[index] = {}\n",
    "for i in intervals:\n",
    "    indices[index][str(i)] = ds[\"sd\"].sel(\n",
    "        time=slice(times[-(i)], times[-1])\n",
    "    ).mean(dim=\"time\")\n",
    "    indices[index][str(i)].name = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23253e5a",
   "metadata": {},
   "source": [
    "### % of Normal SWE\n",
    "\n",
    "So now this one should calculate the percent of the climatologies that the above total precip values represent over the same time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d2d2caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"pnswe\"\n",
    "indices[index] = {}\n",
    "with xr.open_dataset(INPUT_DIR.joinpath(\"era5_swe_climo_81-20.nc\")) as swe_clim_ds:\n",
    "    # need to remap longitude coordinates from [180, 360] to [-180, 0]\n",
    "    swe_clim_ds = swe_clim_ds.assign_coords(longitude=(swe_clim_ds.longitude.values) - 360)\n",
    "    for i in intervals:\n",
    "        # corresponding time slice for climotology dataset\n",
    "        # don't need to do the day-of-year thing since datetimes are provided\n",
    "        #  in this climatology file\n",
    "        # instead, timestamps are for year 1980\n",
    "        clim_time_sl = slice(\n",
    "            f\"1980-{pd.Timestamp(times[-i]).strftime('%m-%d')}\", \n",
    "            f\"1980-{pd.Timestamp(times[-1]).strftime('%m-%d')}\"\n",
    "        )\n",
    "        clim_swe = swe_clim_ds.sel(\n",
    "            time=clim_time_sl,\n",
    "            latitude=swe_clim_ds.latitude.values,\n",
    "        ).mean(dim=\"time\")\n",
    "        indices[index][str(i)] = (clim_swe[\"swe\"] / indices[\"swe\"][str(i)]) * 100\n",
    "        indices[index][str(i)].name = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59ecd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
