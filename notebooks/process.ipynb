{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102b7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from config import DOWNLOAD_DIR, INPUT_DIR, indices_fp\n",
    "import luts\n",
    "# start timer\n",
    "tic = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7decdea",
   "metadata": {},
   "source": [
    "### Merge datasets, create daily sum of total precipitation\n",
    "\n",
    "We need to merge the three (sometimes two?) datasets into a single dataframe.  Then, we need to produce total sums for each day.  Source data will contain a [mix of ERA5 and ERA5T data](https://confluence.ecmwf.int/display/CUSF/ERA5+CDS+requests+which+return+a+mixture+of+ERA5+and+ERA5T+data), so we need to do this to get a consistent set of data:\n",
    " \n",
    "### Our algorithm for merging/fusing data needs to go like this,\n",
    "\n",
    "\n",
    " 1. Some data files may not have `expver` present at all, such as older data which are outside the window of modification for the rolling ERA5 updates.  Use those data as-is.\n",
    " 1. If `expver` has both values 1 and 5 (a mix of ERA5 and ERA5T data), choose `expver == 1` where possible, and `expver == 5` for timesteps when `expver == 1` is missing.\n",
    " 1. For recent data, only `expver` value 5 is present.\n",
    "\n",
    "#### Notes/Questions?\n",
    "\n",
    " * Current month may be only ERA5T / `expver = 5`\n",
    " * Prior months in the same year may have mix of `expver` 5 and 1.\n",
    " * Prior year doesn't have `expver`.\n",
    "\n",
    "\n",
    "Needs to be robust to the presence/absence of expver.\n",
    "\n",
    "Goal is that a dataframe taken for a single point looks correct with data for every timestep (no dupes, no missing).\n",
    "\n",
    "## TODO: make this work properly for January when we may only have 2 data files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f7a67",
   "metadata": {},
   "source": [
    "Define a function to look for the three files for each variable in the input directory and make a complete file of the previous year's worth of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff6c7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_dataset(input_dir, varname):\n",
    "    varname_prefix = luts.varname_prefix_lu[varname]\n",
    "    prior_year = xr.open_dataset(input_dir.joinpath(f\"{varname_prefix}_previous_year.nc\"))\n",
    "    current_year = xr.open_dataset(input_dir.joinpath(f\"{varname_prefix}_current_year.nc\"))\n",
    "    current_month = xr.open_dataset(input_dir.joinpath(f\"{varname_prefix}_current_month.nc\"))\n",
    "    \n",
    "    # Just want to make sure our assumption that data with expver values of 1 and 5 are\n",
    "    #  mutually exclusive and exhaustive\n",
    "    assert np.all(\n",
    "        np.isnan(current_year[varname].sel(expver=5)) == ~np.isnan(current_year[varname].sel(expver=1))\n",
    "    )\n",
    "    assert np.all(\n",
    "        ~np.isnan(current_year[varname].sel(expver=5)) == np.isnan(current_year[varname].sel(expver=1))\n",
    "    )\n",
    "    \n",
    "    # Select the data for each expver value and combine to get a complete continuous set of data:\n",
    "    current_year_fix = xr.merge([\n",
    "        current_year[varname].sel(expver=1).drop(\"expver\"),\n",
    "        current_year[varname].sel(expver=5).drop(\"expver\")\n",
    "    ])\n",
    "    assert ~np.any(np.isnan(current_year_fix[varname]).values)\n",
    "    \n",
    "    # merge with other dataset since they all share the same coordinate variables now\n",
    "    hourly_ds = xr.merge([\n",
    "        prior_year,\n",
    "        current_year_fix,\n",
    "        current_month,\n",
    "    ])\n",
    "    # resample ohurly to daily\n",
    "    if varname in [\"tp\"]:\n",
    "        # total precip should be summed\n",
    "        daily_ds = hourly_ds.resample(time=\"1D\").sum()\n",
    "    elif varname in [\"sd\", \"swvl1\"]:\n",
    "        # TO DO : check how the daily values of these variables should be calculated from hourly\n",
    "        daily_ds = hourly_ds.resample(time=\"1D\").mean()\n",
    "        \n",
    "    return daily_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e4d27",
   "metadata": {},
   "source": [
    "Derive the yearly datasets from the downloads and combine into a single xarray dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9841a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = DOWNLOAD_DIR.joinpath(\"inputs\")\n",
    "\n",
    "ds = xr.combine_by_coords(\n",
    "    [assemble_dataset(input_dir, varname) for varname in [\"tp\", \"sd\"]], \n",
    "    combine_attrs=\"drop_conflicts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522be41e",
   "metadata": {},
   "source": [
    "So we really will only want data as far back as 1 year from most recent data we have (right?), so we can filter those days out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d841cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = ds.time[-1]\n",
    "start_time = ds.time[-365]\n",
    "# ensure that this is indeed 365 days (time diff is nanoseconds)\n",
    "assert (end_time - start_time) / 86400E9\n",
    "ds = ds.sel(time=slice(start_time, end_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d07a76",
   "metadata": {},
   "source": [
    "Now we have a single dataset consisting of all* downloaded variables for deriving all* of the indices.\n",
    "\n",
    "*(not really all of them yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519d6a0",
   "metadata": {},
   "source": [
    "## Indices\n",
    "\n",
    "I know we have another notebook for processing these but we might consider just doing the preprocessing and processing in one go because the preprocessing is rather quick.\n",
    "\n",
    "We will compute the following indices:\n",
    "\n",
    "* SPI - standardized precipitation index\n",
    "* SPEI (can ERA 5 do this?) â€“ standardized potential evaporation index\n",
    "* Total Precip\n",
    "* % of Normal Precipitation\n",
    "* SWE - snow water equivalent\n",
    "* SWE % of Normal \n",
    "* Soil Moisture Deficit\n",
    "\n",
    "These indices will be computed for the following time periods:\n",
    "\n",
    "* 30 day\n",
    "* 60 day\n",
    "* 90 day\n",
    "* 180 day\n",
    "* 365 day\n",
    "\n",
    "We may want to have the 5 time intervals for repeated use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9714360",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = [30, 60 , 90, 180, 365]\n",
    "# also will be referencing the timestamps fairly often\n",
    "times = ds.time.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d39b7",
   "metadata": {},
   "source": [
    "Define a function to query the climatology datasets consistently given the possibility of start/end dates for subsetting straddling the new year. E.g., if start DOY is before end DOY of interval period, then typical slice-based selection is fine. But if end DOY is before (less than) start DOY, e.g. looking back 30 days from Jan 5th, we need to do two selections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443736d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_clim_interval(clim_ds, start_doy, end_doy):\n",
    "    if start_doy < end_doy:\n",
    "        sub_ds = clim_ds.sel(time=slice(start_doy, end_doy))\n",
    "    else:\n",
    "        sub_ds = xr.merge([\n",
    "            clim_ds.sel(time=slice(0, end_doy)),\n",
    "            clim_ds.sel(time=slice(start_doy, 366))\n",
    "        ])\n",
    "        \n",
    "    return sub_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b161419",
   "metadata": {},
   "source": [
    "Also create a dict for writing results maybe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8829050",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eae9c0",
   "metadata": {},
   "source": [
    "### Total precip\n",
    "\n",
    "This one should be easy? Just sum over each time interval?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c64ef4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"tp\"\n",
    "indices[index] = {}\n",
    "for i in intervals:\n",
    "    indices[index][str(i)] = ds[index].sel(\n",
    "        time=slice(times[-(i)], times[-1])\n",
    "    # convert from m to cm to match climatology\n",
    "    ).sum(dim=\"time\") * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e2bb95",
   "metadata": {},
   "source": [
    "###  Total Precip % of Normal\n",
    "\n",
    "So now this one should calculate the percent of the climatologies that the above total precip values represent over the same time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d76b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"pntp\"\n",
    "indices[index] = {}\n",
    "with xr.open_dataset(INPUT_DIR.joinpath(\"era5_daily_tp_climatology_1981_2020_leap.nc\")) as tp_clim_ds:\n",
    "    # need to remap longitude coordinates from [180, 360] to [-180, 0]\n",
    "    tp_clim_ds = tp_clim_ds.assign_coords(longitude=(tp_clim_ds.longitude.values) - 360)\n",
    "    for i in intervals:\n",
    "        start_doy = pd.Timestamp(times[-i]).dayofyear\n",
    "        end_doy = pd.Timestamp(times[-1]).dayofyear\n",
    "        clim_tp = subset_clim_interval(tp_clim_ds, start_doy, end_doy).sum(dim=\"time\")\n",
    "        indices[index][str(i)] = np.round((indices[\"tp\"][str(i)] / clim_tp[\"tp\"]) * 100, 1)\n",
    "        indices[index][str(i)].name = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bec3752",
   "metadata": {},
   "source": [
    "### Snow water equivalent (SWE)\n",
    "\n",
    "#### TO:DO confirm aggregation method over the intervals (currently mean but this is just a guess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f419e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"swe\"\n",
    "\n",
    "indices[index] = {}\n",
    "for i in intervals:\n",
    "    indices[index][str(i)] = ds[\"sd\"].sel(\n",
    "        time=slice(times[-(i)], times[-1])\n",
    "    ).mean(dim=\"time\")\n",
    "    indices[index][str(i)].name = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23253e5a",
   "metadata": {},
   "source": [
    "### SWE % of Normal\n",
    "\n",
    "So now this one should calculate the percent of the climatologies that the above total precip values represent over the same time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2d2caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"pnswe\"\n",
    "indices[index] = {}\n",
    "with xr.open_dataset(INPUT_DIR.joinpath(\"era5_swe_climo_81-20.nc\")) as swe_clim_ds:\n",
    "    # need to remap longitude coordinates from [180, 360] to [-180, 0]\n",
    "    swe_clim_ds = swe_clim_ds.assign_coords(\n",
    "        longitude=(swe_clim_ds.longitude.values) - 360,\n",
    "        # just convert time dim to DOY days for consistency with tp\n",
    "        time=np.arange(swe_clim_ds.time.shape[0]) + 1\n",
    "    )\n",
    "    for i in intervals:\n",
    "        start_doy = pd.Timestamp(times[-i]).dayofyear\n",
    "        end_doy = pd.Timestamp(times[-1]).dayofyear\n",
    "        clim_swe = subset_clim_interval(swe_clim_ds, start_doy, end_doy).mean(dim=\"time\")\n",
    "        indices[index][str(i)] = (indices[\"swe\"][str(i)] / clim_swe[\"swe\"]) * 100\n",
    "        # over the water, SWE will always be zero. This comes out as NaN in the results (the only NaNs)\n",
    "        #  For now just treat this area as 100% of normal.\n",
    "        indices[index][str(i)].values[np.isnan(indices[index][str(i)])] = 100\n",
    "        indices[index][str(i)].name = index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000f3ede",
   "metadata": {},
   "source": [
    "### Merge the indices into a single dataset\n",
    "\n",
    "Since all of the indices will share dimensions / coordinates, we can just put each into an `xarray.Dataset` as a subdataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6fa1a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_ds = xr.merge([\n",
    "    xr.concat(\n",
    "        [indices[varname][key] for key in indices[\"tp\"]], \n",
    "        pd.Index(intervals, name=\"interval\")\n",
    "    )\n",
    "    for varname in indices\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e9cff",
   "metadata": {},
   "source": [
    "Save the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "932550ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline completed in 66s\n"
     ]
    }
   ],
   "source": [
    "indices_ds.to_netcdf(indices_fp)\n",
    "print(f\"Pipeline completed in {round((time.perf_counter() - tic))}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
