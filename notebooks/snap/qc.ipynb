{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b092ae",
   "metadata": {},
   "source": [
    "# Quality Control\n",
    "\n",
    "This notebook is for validating the drought indices dataset produced using the `scripts/process.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a37e58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from config import INDICES_DIR, DOWNLOAD_DIR, CLIM_DIR\n",
    "import luts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79fc57",
   "metadata": {},
   "source": [
    "## Index validation\n",
    "\n",
    "For each drought index, re-compute a value manually and compare with the indices dataset.\n",
    "\n",
    "We will be working with the climatologies, the downloaded ERA5 data, and of course the computed indices data. Set up connections to these datasets.\n",
    "\n",
    "Indices dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbcce76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = pd.Index([1, 7, 30, 60, 90, 180, 365], name=\"interval\")\n",
    "fps = [INDICES_DIR.joinpath(f\"nws_drought_indices_{i}day.nc\") for i in intervals]\n",
    "indices_ds = xr.open_mfdataset(fps, combine=\"nested\", concat_dim=[intervals])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c70658",
   "metadata": {},
   "source": [
    "Define some fixed variables that will be used throughout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fac5d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our most recent day of available data (arbitrary file)\n",
    "with xr.open_dataset(DOWNLOAD_DIR.joinpath(f\"total_precipitation_current_month.nc\")) as ds:\n",
    "    ref_date = ds.time.dt.date.values[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415d469",
   "metadata": {},
   "source": [
    "Define a function to help with extracting data from grid cells in ERA5 downloads, since we will be doing this for every index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "472c56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_era5(index, time_slice, lat, lon):\n",
    "    \"\"\"Function to open the three ERA5 datasets for a given variable name and extract the data from a grid cell for a given point location\"\"\"\n",
    "    varname_lu = {\n",
    "        \"tp\": \"tp\",\n",
    "        \"pntp\": \"tp\",\n",
    "        \"swe\": \"sd\",\n",
    "        \"pnswe\": \"sd\"\n",
    "    }\n",
    "    varname = varname_lu[index]\n",
    "    da_list = []\n",
    "    latlon_sel_di = {\"latitude\": lat, \"longitude\": lon}\n",
    "    for fp in DOWNLOAD_DIR.glob(f\"{luts.varname_prefix_lu[varname]}*.nc\"):\n",
    "        with xr.open_dataset(\n",
    "            fp\n",
    "        ) as ds:\n",
    "            if \"expver\" in ds.dims:\n",
    "                # if expver is present, combine from both into a single dataset and drop it\n",
    "                da = xr.merge([\n",
    "                    ds[varname].sel(\n",
    "                        latlon_sel_di, method=\"nearest\"\n",
    "                    ).sel(expver=1).drop(\"expver\"),\n",
    "                    ds[varname].sel(\n",
    "                        latlon_sel_di, method=\"nearest\"\n",
    "                    ).sel(expver=5).drop(\"expver\")\n",
    "                ])[varname].sel(time=time_slice)\n",
    "\n",
    "            else:\n",
    "                da = ds[varname].sel(\n",
    "                    latlon_sel_di, method=\"nearest\"\n",
    "                ).sel(time=time_slice)\n",
    "\n",
    "            da_list.append(da)\n",
    "            \n",
    "    out_da = xr.concat(da_list, dim=\"time\").sortby(\"time\")\n",
    "    return out_da\n",
    "\n",
    "\n",
    "def extract_clim(varname, doy_slice, lat, lon):\n",
    "    clim_lu = {\n",
    "        \"tp\": \"era5_daily_tp_climatology_1981_2020_leap.nc\",\n",
    "        \"swe\": \"era5_swe_climo_81-20.nc\"\n",
    "    }\n",
    "    with xr.open_dataset(CLIM_DIR.joinpath(clim_lu[varname])) as clim_ds:\n",
    "        \n",
    "        # need to re-index longitude in some cases, not consistent across clim datasets\n",
    "        if clim_ds.longitude.values[0] == 180:\n",
    "            clim_ds = clim_ds.assign_coords(\n",
    "                longitude=(clim_ds.longitude.values) - 360\n",
    "            )\n",
    "        # same reason as above, inconsistency between clims... reindex coords with dayofyear\n",
    "        if clim_ds.time.dt.year.values[0] == 1980:\n",
    "            clim_ds = clim_ds.assign_coords(\n",
    "                time=clim_ds.time.dt.dayofyear\n",
    "            )\n",
    "\n",
    "        clim_da = clim_ds[varname].sel(time=doy_slice).sel(\n",
    "            latitude=lat, longitude=lon, method=\"nearest\"\n",
    "        )\n",
    "\n",
    "    return clim_da\n",
    "\n",
    "    \n",
    "def get_time_slice(ref_date, interval):\n",
    "    start_date = ref_date - pd.to_timedelta(f\"{interval - 1} day\")\n",
    "    return slice(str(start_date), str(ref_date))\n",
    "\n",
    "\n",
    "def get_doy_slice(ref_date, interval):\n",
    "    ref_doy = ref_date.timetuple().tm_yday\n",
    "    start_doy = ref_doy - (interval - 1)\n",
    "    return slice(start_doy, ref_doy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946686be",
   "metadata": {},
   "source": [
    "Now work through each index and test the existing values against newly processed ones for the following intervals and locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d449a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 30\n",
    "lat, lon = 65, -148"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4977a",
   "metadata": {},
   "source": [
    "#### Total precip\n",
    "\n",
    "Total precip should be the sum of the precip values over the specified interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"tp\"\n",
    "test = indices_ds[index].sel(interval=interval).sel(\n",
    "    latitude=lat, longitude=lon, method=\"nearest\"\n",
    ").compute()\n",
    "time_slice = get_time_slice(ref_date, interval)\n",
    "raw = extract_era5(\"tp\", time_slice, lat, lon)\n",
    "# convert m to cm\n",
    "check = (raw.sum() * 100).astype(\"float32\")\n",
    "assert check == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a100b00",
   "metadata": {},
   "source": [
    "#### Total precip % of normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "4022fade",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"pntp\"\n",
    "test = indices_ds[index].sel(interval=interval).sel(\n",
    "    latitude=lat, longitude=lon, method=\"nearest\"\n",
    ").compute()\n",
    "doy_slice = get_doy_slice(ref_date, interval)\n",
    "clim = extract_clim(\"tp\", doy_slice, lat, lon)\n",
    "check = np.round(((raw.sum() * 100) / clim.sum()).astype(\"float32\") * 100)\n",
    "assert check == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544067f",
   "metadata": {},
   "source": [
    "#### Snow water equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "848e7ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"swe\"\n",
    "test = indices_ds[index].sel(interval=interval).sel(\n",
    "    latitude=lat, longitude=lon, method=\"nearest\"\n",
    ").compute()\n",
    "time_slice = get_time_slice(ref_date, interval)\n",
    "raw = extract_era5(index, time_slice, lat, lon)\n",
    "check = (raw.mean() * 100).astype(\"float32\")\n",
    "assert check == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a72fd7",
   "metadata": {},
   "source": [
    "#### SWE % of normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "7fd60996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "index = \"pnswe\"\n",
    "test = indices_ds[index].sel(interval=interval).sel(\n",
    "    latitude=lat, longitude=lon, method=\"nearest\"\n",
    ").compute()\n",
    "doy_slice = get_doy_slice(ref_date, interval)\n",
    "clim = extract_clim(\"swe\", doy_slice, lat, lon)\n",
    "check = np.round((raw.mean() / clim.mean()) * 100).astype(\"float32\")\n",
    "assert check == test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61d1fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
