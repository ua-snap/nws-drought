{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b092ae",
   "metadata": {},
   "source": [
    "# Quality Control\n",
    "\n",
    "This notebook is for validating the drought indices dataset produced using the `scripts/process.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a37e58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from config import INDICES_DIR, DOWNLOAD_DIR, CLIM_DIR\n",
    "import luts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79fc57",
   "metadata": {},
   "source": [
    "## Index validation\n",
    "\n",
    "For each drought index, re-compute a value manually and compare with the indices dataset.\n",
    "\n",
    "We will be working with the climatologies, the downloaded ERA5 data, and of course the computed indices data. Set up connections to these datasets.\n",
    "\n",
    "Indices dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbcce76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = pd.Index([1, 7, 30, 60, 90, 180, 365], name=\"interval\")\n",
    "fps = [INDICES_DIR.joinpath(f\"nws_drought_indices_{i}day.nc\") for i in intervals]\n",
    "indices_ds = xr.open_mfdataset(fps, combine=\"nested\", concat_dim=[intervals])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c70658",
   "metadata": {},
   "source": [
    "Define some fixed variables that will be used throughout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fac5d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our most recent day of available data (arbitrary file)\n",
    "with xr.open_dataset(DOWNLOAD_DIR.joinpath(f\"total_precipitation_current_month.nc\")) as ds:\n",
    "    ref_date = ds.time.dt.date.values[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415d469",
   "metadata": {},
   "source": [
    "Define a function to help with extracting data from grid cells in ERA5 downloads, since we will be doing this for every index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "472c56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_era5(varname, time_slice, lat, lon):\n",
    "    \"\"\"Function to open the three ERA5 datasets for a given variable name and extract the data from a grid cell for a given point location\"\"\"\n",
    "    da_list = []\n",
    "    latlon_sel_di = {\"latitude\": lat, \"longitude\": lon}\n",
    "    for fp in DOWNLOAD_DIR.glob(f\"{luts.varname_prefix_lu[varname]}*.nc\"):\n",
    "        with xr.open_dataset(\n",
    "            fp\n",
    "        ) as ds:\n",
    "            if \"expver\" in ds.dims:\n",
    "                # if expver is present, combine from both into a single dataset and drop it\n",
    "                da = xr.merge([\n",
    "                    ds[varname].sel(\n",
    "                        latlon_sel_di, method=\"nearest\"\n",
    "                    ).sel(expver=1).drop(\"expver\"),\n",
    "                    ds[varname].sel(\n",
    "                        latlon_sel_di, method=\"nearest\"\n",
    "                    ).sel(expver=5).drop(\"expver\")\n",
    "                ])[varname].sel(time=time_slice)\n",
    "\n",
    "            else:\n",
    "                da = ds[varname].sel(\n",
    "                    latlon_sel_di, method=\"nearest\"\n",
    "                ).sel(time=time_slice)\n",
    "\n",
    "            da_list.append(da)\n",
    "            \n",
    "    out_da = xr.concat(da_list, dim=\"time\").sortby(\"time\")\n",
    "    return out_da\n",
    "\n",
    "    \n",
    "def get_time_slice(ref_date, interval):\n",
    "    start_date = ref_date - pd.to_timedelta(f\"{interval - 1} day\")\n",
    "    return slice(str(start_date), str(ref_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4977a",
   "metadata": {},
   "source": [
    "Now work through each index and test the existing values against newly processed ones.\n",
    "\n",
    "#### Total precip\n",
    "\n",
    "Total precip should be the sum of the precip values over the specified interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06b43a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = \"tp\"\n",
    "interval = 30\n",
    "lat, lon = 65, -148\n",
    "time_slice = get_time_slice(ref_date, interval)\n",
    "\n",
    "test = indices_ds[\"tp\"].sel(interval=interval).sel(\n",
    "    latitude=lat, longitude=lon, method=\"nearest\"\n",
    ").compute()\n",
    "raw = extract_era5(\"tp\", time_slice, lat, lon)\n",
    "\n",
    "# convert m to cm\n",
    "assert (raw.sum() * 100).astype(\"float32\") == test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d4f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
