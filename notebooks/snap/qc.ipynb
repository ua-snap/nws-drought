{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b092ae",
   "metadata": {},
   "source": [
    "# Quality Control\n",
    "\n",
    "This notebook is for validating the drought indices dataset produced using the `scripts/process.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37e58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from config import INDICES_DIR, DOWNLOAD_DIR, CLIM_DIR\n",
    "import luts\n",
    "from scripts.snap.process_calibration_params import estimate_params\n",
    "from xclim.indices.stats import dist_method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79fc57",
   "metadata": {},
   "source": [
    "## Index validation\n",
    "\n",
    "For each drought index, re-compute a value manually and compare with the indices dataset.\n",
    "\n",
    "We will be working with the climatologies, the downloaded ERA5 data, and of course the computed indices data. Set up connections to these datasets.\n",
    "\n",
    "Indices dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbcce76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = pd.Index([1, 7, 30, 60, 90, 180, 365], name=\"interval\")\n",
    "fps = [INDICES_DIR.joinpath(f\"nws_drought_indices_{i}day.nc\") for i in intervals]\n",
    "indices_ds = xr.open_mfdataset(fps, combine=\"nested\", concat_dim=[intervals])\n",
    "\n",
    "# get the refernce date\n",
    "ref_date = pd.to_datetime(indices_ds.attrs[\"reference_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415d469",
   "metadata": {},
   "source": [
    "Define a function to help with extracting data from grid cells in ERA5 downloads, since we will be doing this for every index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "472c56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_era5(index, time_slice, lat, lon):\n",
    "    \"\"\"Function to open the three ERA5 datasets for a given variable name and extract the data from a grid cell for a given point location\"\"\"\n",
    "    varname_lu = {\n",
    "        \"tp\": \"tp\",\n",
    "        \"pntp\": \"tp\",\n",
    "        \"swe\": \"sd\",\n",
    "        \"pnswe\": \"sd\",\n",
    "        \"spi\": \"tp\",\n",
    "        \"smd1\": \"swvl1\",\n",
    "        \"smd2\": \"swvl2\",\n",
    "        \"pev\": \"pev\"\n",
    "    }\n",
    "    varname = varname_lu[index]\n",
    "    da_list = []\n",
    "    latlon_sel_di = {\"latitude\": lat, \"longitude\": lon}\n",
    "    for fp in DOWNLOAD_DIR.glob(f\"{luts.varname_prefix_lu[varname]}*.nc\"):\n",
    "        with xr.open_dataset(fp) as ds:\n",
    "            if \"expver\" in ds.dims:\n",
    "                # if expver is present, combine from both into a single dataset and drop it\n",
    "                da = xr.merge([\n",
    "                    ds[varname].sel(\n",
    "                        latlon_sel_di, method=\"nearest\"\n",
    "                    ).sel(expver=1).drop(\"expver\"),\n",
    "                    ds[varname].sel(\n",
    "                        latlon_sel_di, method=\"nearest\"\n",
    "                    ).sel(expver=5).drop(\"expver\")\n",
    "                ])[varname].sel(time=time_slice)\n",
    "\n",
    "            else:\n",
    "                da = ds[varname].sel(\n",
    "                    latlon_sel_di, method=\"nearest\"\n",
    "                ).sel(time=time_slice)\n",
    "\n",
    "            da_list.append(da)\n",
    "            \n",
    "    out_da = xr.concat(da_list, dim=\"time\").sortby(\"time\")\n",
    "    return out_da\n",
    "\n",
    "\n",
    "def extract_clim(varname, doy_list, lat, lon):\n",
    "    clim_lu = {\n",
    "        \"tp\": \"era5_daily_tp_climatology_1981_2020_leap.nc\",\n",
    "        \"swe\": \"era5_swe_climo_81-20.nc\",\n",
    "        \"swvl\": \"era5_daily_swvl_1981_2020.nc\"\n",
    "    }\n",
    "    with xr.open_dataset(CLIM_DIR.joinpath(clim_lu[varname])) as clim_ds:\n",
    "        \n",
    "        # need to re-index longitude in some cases, not consistent across clim datasets\n",
    "        if clim_ds.longitude.values[0] == 180:\n",
    "            clim_ds = clim_ds.assign_coords(\n",
    "                longitude=(clim_ds.longitude.values) - 360\n",
    "            )\n",
    "        # same reason as above, inconsistency between clims... reindex coords with dayofyear\n",
    "        try:\n",
    "            if clim_ds.time.dt.year.values[0] == 1980:\n",
    "                clim_ds = clim_ds.assign_coords(\n",
    "                    time=clim_ds.time.dt.dayofyear\n",
    "                )\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        clim_da = clim_ds[varname].sel(time=doy_list).sel(\n",
    "            latitude=lat, longitude=lon, method=\"nearest\"\n",
    "        )\n",
    "\n",
    "    return clim_da\n",
    "\n",
    "    \n",
    "def get_time_slice(ref_date, interval):\n",
    "    start_date = ref_date - pd.to_timedelta(f\"{interval - 1} day\")\n",
    "    return slice(start_date.strftime(\"%Y-%m-%d\"), ref_date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "\n",
    "def get_doy_list(ref_date, interval):\n",
    "    ref_doy = ref_date.timetuple().tm_yday\n",
    "    start_doy = ref_doy - (interval - 1)\n",
    "    if start_doy < 1:\n",
    "        second_start = 365 - (-start_doy)\n",
    "        doys = list(\n",
    "            np.arange(1, ref_doy + 1)\n",
    "        ) + list(np.arange(second_start, 367))\n",
    "    else:\n",
    "        doys = list(np.arange(start_doy, ref_doy + 1))\n",
    "    return doys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982fc84b",
   "metadata": {},
   "source": [
    "Okay the gameplan here will be to define a function that checks some index (or index precursor in the case of SPI and SPEI gamma parameters) for some global lat/lon and interval (using globals for these as they will be used by every function).\n",
    "\n",
    "We will define the functions and then iterate over some coordinate and interval combinations and compare against extractions from existing data. Some functions will return some output for use in other functions where there are useful dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9be1e3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tp():\n",
    "    index = \"tp\"\n",
    "    test = indices_ds[index].sel(interval=interval).sel(\n",
    "        **latlon_sel_di\n",
    "    ).compute()\n",
    "    raw = extract_era5(\"tp\", time_slice, lat, lon)\n",
    "    # convert m to cm\n",
    "    check = np.round((raw.sum() * 100).astype(\"float32\"), 1)\n",
    "    # save value for pntp check\n",
    "    tp_check = check\n",
    "    assert check == test\n",
    "    \n",
    "    return tp_check\n",
    "\n",
    "\n",
    "def check_pntp(tp_check):\n",
    "    index = \"pntp\"\n",
    "    test = indices_ds[index].sel(interval=interval).sel(\n",
    "        **latlon_sel_di\n",
    "    ).compute()\n",
    "    clim = extract_clim(\"tp\", doy_list, lat, lon)\n",
    "    check = np.round(((tp_check / clim.sum()) * 100).astype(\"float32\"), 1)\n",
    "    \n",
    "    assert check == test\n",
    "    \n",
    "    \n",
    "def check_swe():\n",
    "    index = \"swe\"\n",
    "    test = indices_ds[index].sel(interval=interval).sel(\n",
    "        **latlon_sel_di\n",
    "    ).compute()\n",
    "    raw = extract_era5(index, time_slice, lat, lon)\n",
    "    check = np.round((raw.mean() * 100).astype(\"float32\"), 1)\n",
    "    swe_check = check\n",
    "    assert check == test\n",
    "    \n",
    "    return swe_check\n",
    "\n",
    "\n",
    "def check_pnswe(swe_check):\n",
    "    index = \"pnswe\"\n",
    "    test = indices_ds[index].sel(interval=interval).sel(\n",
    "        **latlon_sel_di\n",
    "    ).compute()\n",
    "    clim = extract_clim(\"swe\", doy_list, lat, lon)\n",
    "    check = np.round((swe_check / (clim.mean())), 1).astype(\"float32\")\n",
    "\n",
    "    assert check == test\n",
    "    \n",
    "    \n",
    "def check_smd():\n",
    "    index = \"smd\"\n",
    "    test = indices_ds[index].sel(interval=interval).sel(\n",
    "        **latlon_sel_di\n",
    "    ).compute()\n",
    "    # two different extractions since we are working with two different \n",
    "    #  levels here\n",
    "    raw1 = extract_era5(index + \"1\", time_slice, lat, lon)\n",
    "    raw2 = extract_era5(index + \"2\", time_slice, lat, lon)\n",
    "    #combine the levels\n",
    "    raw = (raw1 * 0.25) + (raw2 * 0.75)\n",
    "    clim = extract_clim(\"swvl\", doy_list, lat, lon)\n",
    "    check = np.round((((clim.mean() - raw.mean()) / clim.mean()) * 100).astype(\"float32\"), 1)\n",
    "\n",
    "    assert check == test\n",
    "    \n",
    "    \n",
    "# Okay for these two indices we need to do little bit more checking.\n",
    "# We have the daily precip and pev files at \n",
    "# `/workspace/Shared/Tech_Projects/NWS_Drought_Indicators/project_data/calibration/`. \n",
    "# We will first validate the estimated parameters of the gamma distributions\n",
    "# fit to these daily data by recomputing them from the daily data.\n",
    "\n",
    "def check_spi_params():\n",
    "    # copied from /workspace/Shared/Tech_Projects/NWS_Drought_Indicators/project_data/calibration\n",
    "    cal_dir = Path(\"/atlas_scratch/kmredilla/nws_drought_indicators/calibration/\")\n",
    "    tp_daily_fp = cal_dir.joinpath(\"era5_daily_tp_1981_2020.nc\")\n",
    "    daily_tp_ds = xr.open_dataset(tp_daily_fp)\n",
    "    daily_tp = daily_tp_ds[\"tp\"].sel(**latlon_sel_di)\n",
    "    spi_params = estimate_params(daily_tp, interval)\n",
    "    check = spi_params.sel(dayofyear=ref_doy).astype(\"float32\")\n",
    "\n",
    "    with xr.open_dataset(CLIM_DIR.joinpath(\"spi_gamma_parameters.nc\")) as spi_ds:\n",
    "        test = spi_ds[\"params\"].sel(**latlon_sel_di).sel(\n",
    "            interval=interval, dayofyear=ref_doy\n",
    "        )\n",
    "\n",
    "    assert np.all(check == test)\n",
    "    return daily_tp, spi_params\n",
    "\n",
    "\n",
    "def check_spei_params(daily_tp):\n",
    "    # copied from /workspace/Shared/Tech_Projects/NWS_Drought_Indicators/project_data/calibration\n",
    "    cal_dir = Path(\"/atlas_scratch/kmredilla/nws_drought_indicators/calibration/\")\n",
    "    pev_daily_fp = cal_dir.joinpath(\"era5_daily_pev_1981_2020.nc\")\n",
    "    daily_pev_ds = xr.open_dataset(pev_daily_fp)\n",
    "    daily_pev = daily_pev_ds[\"pev\"].sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "    daily_wb = daily_tp + daily_pev\n",
    "    daily_wb += 0.002\n",
    "    spei_params = estimate_params(daily_wb, interval)\n",
    "    check = spei_params.sel(dayofyear=ref_doy).astype(\"float32\")\n",
    "\n",
    "    with xr.open_dataset(CLIM_DIR.joinpath(\"spei_gamma_parameters.nc\")) as spei_ds:\n",
    "        test = spei_ds[\"params\"].sel(**latlon_sel_di).sel(\n",
    "            interval=interval, dayofyear=ref_doy\n",
    "        )\n",
    "\n",
    "    assert np.all(check == test)\n",
    "    return spei_params\n",
    "\n",
    "\n",
    "def check_spi(spi_params):\n",
    "    index = \"spi\"\n",
    "    test = indices_ds[index].sel(interval=interval).sel(\n",
    "        **latlon_sel_di\n",
    "    ).compute()\n",
    "    raw_tp = extract_era5(\"tp\", time_slice, lat, lon)\n",
    "    pr = raw_tp.resample(time=\"1D\").sum().mean()\n",
    "    spi_params.attrs[\"scipy_dist\"] = \"gamma\"\n",
    "\n",
    "    # do the statistical parts\n",
    "    prob = dist_method(\n",
    "        \"cdf\",\n",
    "        spi_params.sel(dayofyear=ref_doy).astype(\"float32\"),\n",
    "        pr.where(pr > 0)\n",
    "    )\n",
    "    params_norm = xr.DataArray(\n",
    "        [0, 1],\n",
    "        dims=[\"dparams\"],\n",
    "        coords=dict(dparams=([\"loc\", \"scale\"])),\n",
    "        attrs=dict(scipy_dist=\"norm\"),\n",
    "    )\n",
    "    check = np.round(dist_method(\"ppf\", params_norm, prob), 2)\n",
    "\n",
    "    assert check == test\n",
    "    return raw_tp\n",
    "\n",
    "\n",
    "def check_spei(raw_tp, spei_params):\n",
    "    index = \"spei\"\n",
    "    test = indices_ds[index].sel(interval=interval).sel(\n",
    "        **latlon_sel_di\n",
    "    ).compute()\n",
    "    raw_pev = extract_era5(\"pev\", time_slice, lat, lon)\n",
    "    wb = (raw_tp + raw_pev).resample(time=\"1D\").sum().mean()\n",
    "    wb += 0.002\n",
    "    spei_params.attrs[\"scipy_dist\"] = \"gamma\"\n",
    "\n",
    "    # do the statistical parts\n",
    "    prob = dist_method(\n",
    "        \"cdf\",\n",
    "        spei_params.sel(dayofyear=ref_doy).astype(\"float32\"),\n",
    "        wb.where(wb > 0)\n",
    "    )\n",
    "    params_norm = xr.DataArray(\n",
    "        [0, 1],\n",
    "        dims=[\"dparams\"],\n",
    "        coords=dict(dparams=([\"loc\", \"scale\"])),\n",
    "        attrs=dict(scipy_dist=\"norm\"),\n",
    "    )\n",
    "    check = np.round(dist_method(\"ppf\", params_norm, prob), 2)\n",
    "\n",
    "    assert check == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a4a9a8",
   "metadata": {},
   "source": [
    "Okay, now the magic hopefully happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccb58a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.7 s, sys: 12.6 s, total: 1min 1s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "intervals = [7, 30, 60, 90, 180, 365]\n",
    "coords =  ((65, -148), (61, -150), (58, -134), (71, -157))\n",
    "ref_doy = ref_date.timetuple().tm_yday\n",
    "for interval in intervals:\n",
    "    time_slice = get_time_slice(ref_date, interval)\n",
    "    doy_list = get_doy_list(ref_date, interval)\n",
    "    for coord in coords:\n",
    "        lat, lon = coord\n",
    "        latlon_sel_di = {\"latitude\": lat, \"longitude\": lon, \"method\": \"nearest\"}\n",
    "        tp_check = check_tp()\n",
    "        check_pntp(tp_check)\n",
    "        swe_check = check_swe()\n",
    "        check_pnswe(swe_check)\n",
    "        check_smd()\n",
    "        daily_tp, spi_params = check_spi_params()\n",
    "        spei_params = check_spei_params(daily_tp)\n",
    "        raw_tp = check_spi(spi_params)\n",
    "        check_spei(raw_tp, spei_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40796567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
