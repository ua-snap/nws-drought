{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86b092ae",
   "metadata": {},
   "source": [
    "# Quality Control\n",
    "\n",
    "This notebook is for validating the drought indices dataset produced using the `scripts/process.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37e58b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from config import INDICES_DIR, DOWNLOAD_DIR, CLIM_DIR\n",
    "import luts\n",
    "from scripts.snap.process_calibration_params import estimate_params\n",
    "from xclim.indices.stats import dist_method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba79fc57",
   "metadata": {},
   "source": [
    "## Index validation\n",
    "\n",
    "For each drought index, re-compute a value manually and compare with the indices dataset.\n",
    "\n",
    "We will be working with the climatologies, the downloaded ERA5 data, and of course the computed indices data. Set up connections to these datasets.\n",
    "\n",
    "Indices dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbcce76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = pd.Index([1, 7, 30, 60, 90, 180, 365], name=\"interval\")\n",
    "fps = [INDICES_DIR.joinpath(f\"nws_drought_indices_{i}day.nc\") for i in intervals]\n",
    "indices_ds = xr.open_mfdataset(fps, combine=\"nested\", concat_dim=[intervals])\n",
    "\n",
    "# get the refernce date\n",
    "ref_date = pd.to_datetime(indices_ds.attrs[\"reference_date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415d469",
   "metadata": {},
   "source": [
    "Define a function to help with extracting data from grid cells in ERA5 downloads, since we will be doing this for every index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "472c56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_era5(index, time_slice, lat, lon):\n",
    "    \"\"\"Function to open the three ERA5 datasets for a given variable name and extract the data from a grid cell for a given point location\"\"\"\n",
    "    varname_lu = {\n",
    "        \"tp\": \"tp\",\n",
    "        \"pntp\": \"tp\",\n",
    "        \"swe\": \"sd\",\n",
    "        \"pnswe\": \"sd\",\n",
    "        \"spi\": \"tp\",\n",
    "        \"smd1\": \"swvl1\",\n",
    "        \"smd2\": \"swvl2\",\n",
    "        \"pev\": \"pev\"\n",
    "    }\n",
    "    varname = varname_lu[index]\n",
    "    da_list = []\n",
    "    latlon_sel_di = {\"latitude\": lat, \"longitude\": lon}\n",
    "    for fp in DOWNLOAD_DIR.glob(f\"{luts.varname_prefix_lu[varname]}*.nc\"):\n",
    "        with xr.open_dataset(fp) as ds:\n",
    "            if \"expver\" in ds.dims:\n",
    "                # if expver is present, combine from both into a single dataset and drop it\n",
    "                da = xr.merge([\n",
    "                    ds[varname].sel(\n",
    "                        latlon_sel_di, method=\"nearest\"\n",
    "                    ).sel(expver=1).drop(\"expver\"),\n",
    "                    ds[varname].sel(\n",
    "                        latlon_sel_di, method=\"nearest\"\n",
    "                    ).sel(expver=5).drop(\"expver\")\n",
    "                ])[varname].sel(time=time_slice)\n",
    "\n",
    "            else:\n",
    "                da = ds[varname].sel(\n",
    "                    latlon_sel_di, method=\"nearest\"\n",
    "                ).sel(time=time_slice)\n",
    "\n",
    "            da_list.append(da)\n",
    "            \n",
    "    out_da = xr.concat(da_list, dim=\"time\").sortby(\"time\")\n",
    "    return out_da\n",
    "\n",
    "\n",
    "def extract_clim(varname, doy_slice, lat, lon):\n",
    "    clim_lu = {\n",
    "        \"tp\": \"era5_daily_tp_climatology_1981_2020_leap.nc\",\n",
    "        \"swe\": \"era5_swe_climo_81-20.nc\",\n",
    "        \"swvl\": \"era5_daily_swvl_1981_2020.nc\"\n",
    "    }\n",
    "    with xr.open_dataset(CLIM_DIR.joinpath(clim_lu[varname])) as clim_ds:\n",
    "        \n",
    "        # need to re-index longitude in some cases, not consistent across clim datasets\n",
    "        if clim_ds.longitude.values[0] == 180:\n",
    "            clim_ds = clim_ds.assign_coords(\n",
    "                longitude=(clim_ds.longitude.values) - 360\n",
    "            )\n",
    "        # same reason as above, inconsistency between clims... reindex coords with dayofyear\n",
    "        try:\n",
    "            if clim_ds.time.dt.year.values[0] == 1980:\n",
    "                clim_ds = clim_ds.assign_coords(\n",
    "                    time=clim_ds.time.dt.dayofyear\n",
    "                )\n",
    "        except TypeError:\n",
    "            pass\n",
    "\n",
    "        clim_da = clim_ds[varname].sel(time=doy_slice).sel(\n",
    "            latitude=lat, longitude=lon, method=\"nearest\"\n",
    "        )\n",
    "\n",
    "    return clim_da\n",
    "\n",
    "    \n",
    "def get_time_slice(ref_date, interval):\n",
    "    start_date = ref_date - pd.to_timedelta(f\"{interval - 1} day\")\n",
    "    return slice(start_date.strftime(\"%Y-%m-%d\"), ref_date.strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "\n",
    "def get_doy_slice(ref_date, interval):\n",
    "    ref_doy = ref_date.timetuple().tm_yday\n",
    "    start_doy = ref_doy - (interval - 1)\n",
    "    return slice(start_doy, ref_doy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982fc84b",
   "metadata": {},
   "source": [
    "Now work through each index and test the existing values against newly processed ones for the following intervals and locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a449c3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "interval = 30\n",
    "lat, lon = 65, -148\n",
    "latlon_sel_di = {\"latitude\": lat, \"longitude\": lon, \"method\": \"nearest\"}\n",
    "ref_doy = ref_date.timetuple().tm_yday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4977a",
   "metadata": {},
   "source": [
    "#### Total precip\n",
    "\n",
    "Total precip should be the sum of the precip values over the specified interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8355c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"tp\"\n",
    "test = indices_ds[index].sel(interval=interval).sel(\n",
    "    latitude=lat, longitude=lon, method=\"nearest\"\n",
    ").compute()\n",
    "time_slice = get_time_slice(ref_date, interval)\n",
    "raw = extract_era5(\"tp\", time_slice, lat, lon)\n",
    "# convert m to cm\n",
    "check = np.round((raw.sum() * 100).astype(\"float32\"), 1)\n",
    "# save value for pntp check\n",
    "tp_check = check\n",
    "assert check == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2eaf1",
   "metadata": {},
   "source": [
    "#### Total precip % of normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f66756d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"pntp\"\n",
    "test = indices_ds[index].sel(interval=interval).sel(\n",
    "    latitude=lat, longitude=lon, method=\"nearest\"\n",
    ").compute()\n",
    "doy_slice = get_doy_slice(ref_date, interval)\n",
    "clim = extract_clim(\"tp\", doy_slice, lat, lon)\n",
    "check = np.round((tp_check / clim.sum()).astype(\"float32\") * 100)\n",
    "assert check == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9805b29f",
   "metadata": {},
   "source": [
    "#### Snow water equivalent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5d7010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"swe\"\n",
    "test = indices_ds[index].sel(interval=interval).sel(\n",
    "    latitude=lat, longitude=lon, method=\"nearest\"\n",
    ").compute()\n",
    "time_slice = get_time_slice(ref_date, interval)\n",
    "raw = extract_era5(index, time_slice, lat, lon)\n",
    "check = np.round((raw.mean() * 100).astype(\"float32\"), 1)\n",
    "swe_check = check\n",
    "assert check == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6dd51b",
   "metadata": {},
   "source": [
    "#### SWE % of normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b0e3dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"pnswe\"\n",
    "test = indices_ds[index].sel(interval=interval).sel(\n",
    "    latitude=lat, longitude=lon, method=\"nearest\"\n",
    ").compute()\n",
    "doy_slice = get_doy_slice(ref_date, interval)\n",
    "clim = extract_clim(\"swe\", doy_slice, lat, lon)\n",
    "check = np.round((swe_check / clim.mean()), 1).astype(\"float32\")\n",
    "assert check == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758113cc",
   "metadata": {},
   "source": [
    "#### Soil moisture deficit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d050e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"smd\"\n",
    "test = indices_ds[index].sel(interval=interval).sel(\n",
    "    latitude=lat, longitude=lon, method=\"nearest\"\n",
    ").compute()\n",
    "time_slice = get_time_slice(ref_date, interval)\n",
    "# two different extractions since we are working with two different \n",
    "#  levels here\n",
    "raw1 = extract_era5(index + \"1\", time_slice, lat, lon)\n",
    "raw2 = extract_era5(index + \"2\", time_slice, lat, lon)\n",
    "#combine the levels\n",
    "raw = (raw1 * 0.25) + (raw2 * 0.75)\n",
    "doy_slice = get_doy_slice(ref_date, interval)\n",
    "clim = extract_clim(\"swvl\", doy_slice, lat, lon)\n",
    "check = np.round((((clim.mean() - raw.mean()) / clim.mean()) * 100).astype(\"float32\"), 1)\n",
    "assert check == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b06b177",
   "metadata": {},
   "source": [
    "#### SPI & SPEI gamma parameters\n",
    "\n",
    "Okay for these two indices we need to do little bit more checking. We have the daily precip and pev files at `/workspace/Shared/Tech_Projects/NWS_Drought_Indicators/project_data/calibration/`. We will first validate the estimated parameters of the gamma distributions fit to these daily data by recomputing them from the daily data.\n",
    "\n",
    "##### SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c584df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from /workspace/Shared/Tech_Projects/NWS_Drought_Indicators/project_data/calibration\n",
    "cal_dir = Path(\"/atlas_scratch/kmredilla/nws_drought_indicators/calibration/\")\n",
    "\n",
    "tp_daily_fp = cal_dir.joinpath(\"era5_daily_tp_1981_2020.nc\")\n",
    "daily_tp_ds = xr.open_dataset(tp_daily_fp)\n",
    "daily_tp = daily_tp_ds[\"tp\"].sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "spi_params = estimate_params(daily_tp, interval)\n",
    "check = spi_params.sel(dayofyear=ref_doy).astype(\"float32\")\n",
    "\n",
    "with xr.open_dataset(CLIM_DIR.joinpath(\"spi_gamma_parameters.nc\")) as spi_ds:\n",
    "    test = spi_ds[\"params\"].sel(\n",
    "        interval=interval, latitude=lat, longitude=lon, dayofyear=ref_doy\n",
    "    )\n",
    "\n",
    "assert np.all(check == test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374c9c50",
   "metadata": {},
   "source": [
    "##### SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4f95c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 768 ms, sys: 1.26 s, total: 2.03 s\n",
      "Wall time: 6.97 s\n"
     ]
    }
   ],
   "source": [
    "pev_daily_fp = cal_dir.joinpath(\"era5_daily_pev_1981_2020.nc\")\n",
    "daily_pev_ds = xr.open_dataset(pev_daily_fp)\n",
    "daily_pev = daily_pev_ds[\"pev\"].sel(latitude=lat, longitude=lon, method=\"nearest\")\n",
    "daily_wb = daily_tp + daily_pev\n",
    "daily_wb += 0.002\n",
    "spei_params = estimate_params(daily_wb, interval)\n",
    "check = spei_params.sel(dayofyear=ref_doy).astype(\"float32\")\n",
    "\n",
    "with xr.open_dataset(CLIM_DIR.joinpath(\"spei_gamma_parameters.nc\")) as spei_ds:\n",
    "    test = spei_ds[\"params\"].sel(\n",
    "        interval=interval, latitude=lat, longitude=lon, dayofyear=ref_doy\n",
    "    )\n",
    "\n",
    "assert np.all(check == test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d16722",
   "metadata": {},
   "source": [
    "#### SPI & SPEI\n",
    "\n",
    "Now we can use the gamma parameters we just validated and compute the indices.\n",
    "\n",
    "##### SPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d283f9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"spi\"\n",
    "test = indices_ds[index].sel(interval=interval).sel(\n",
    "    **latlon_sel_di\n",
    ").compute()\n",
    "time_slice = get_time_slice(ref_date, interval)\n",
    "raw_tp = extract_era5(\"tp\", time_slice, lat, lon)\n",
    "pr = raw_tp.resample(time=\"1D\").sum().mean()\n",
    "spi_params.attrs[\"scipy_dist\"] = \"gamma\"\n",
    "\n",
    "# do the statistical parts\n",
    "prob = dist_method(\n",
    "    \"cdf\",\n",
    "    spi_params.sel(dayofyear=ref_doy).astype(\"float32\"),\n",
    "    pr.where(pr > 0)\n",
    ")\n",
    "params_norm = xr.DataArray(\n",
    "    [0, 1],\n",
    "    dims=[\"dparams\"],\n",
    "    coords=dict(dparams=([\"loc\", \"scale\"])),\n",
    "    attrs=dict(scipy_dist=\"norm\"),\n",
    ")\n",
    "check = np.round(dist_method(\"ppf\", params_norm, prob), 2)\n",
    "\n",
    "assert check == test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9e505",
   "metadata": {},
   "source": [
    "##### SPEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9950320",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"spei\"\n",
    "test = indices_ds[index].sel(interval=interval).sel(\n",
    "    **latlon_sel_di\n",
    ").compute()\n",
    "time_slice = get_time_slice(ref_date, interval)\n",
    "raw_pev = extract_era5(\"pev\", time_slice, lat, lon)\n",
    "wb = (raw_tp + raw_pev).resample(time=\"1D\").sum().mean()\n",
    "wb += 0.002\n",
    "spei_params.attrs[\"scipy_dist\"] = \"gamma\"\n",
    "\n",
    "# do the statistical parts\n",
    "prob = dist_method(\n",
    "    \"cdf\",\n",
    "    spei_params.sel(dayofyear=ref_doy).astype(\"float32\"),\n",
    "    wb.where(wb > 0)\n",
    ")\n",
    "params_norm = xr.DataArray(\n",
    "    [0, 1],\n",
    "    dims=[\"dparams\"],\n",
    "    coords=dict(dparams=([\"loc\", \"scale\"])),\n",
    "    attrs=dict(scipy_dist=\"norm\"),\n",
    ")\n",
    "check = np.round(dist_method(\"ppf\", params_norm, prob), 2)\n",
    "\n",
    "assert check == test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28928eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
